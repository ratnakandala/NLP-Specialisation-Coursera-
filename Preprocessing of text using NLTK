import nltk
from nltk.corpus import twitter_samples
import matplotlib.pyplot as plt
import random  #to generate pseudo-random number

nltk.download('twitter_samples') #downloading the sample twitter dataset

all_positive_tweets =  twitter_samples.strings('positive_tweets.json')  #select all the positive tweets
all_negative_tweets = twitter_samples.strings('negative_tweets.json')  #select all the negative tweets

print('Number of positive tweets: ', len(all_positive_tweets))  #print the number of positive tweets
print('Number of negative tweets: ', len(all_negative_tweets))  #print the number of negative tweets

print('\n The type of all_positive_tweets is: ',type(all_positive_tweets)) #print the type of positive tweets #'\n': to start on a new line
print('The type of all_negative_tweets is: ', type(all_negative_tweets))    #print the type of negative tweets

#PLOT THE INFO IN A PIE CHART
fig = plt.figure(figsize=(5,5))  #Declare a new figure with its size
labels = 'Positives', 'Negative'  #Labels for the two classes
sizes = [len(all_positive_tweets), len(all_negative_tweets)]   #sizes 
plt.pie(sizes, labels = labels, autopct = '%1.1f%%', shadow = True, startangle = 90)  #Declare pie chart, where the slices will be ordered and plotted counter-clockwise


plt.axis('equal')  #Equal aspect ratio ensures that pie is drawn as a circle

plt.show() #Display the chart


#PRINTING RANDOM RAW TWEETS BEFORE PREPROCESSING; different color for each type

print('\033[92m' +  all_positive_tweets[random.randint(0,5000)])  #print the random positive tweet in green
print('\033[91m' + all_negative_tweets[random.randint(0, 5000)])  #print the random negative tweet in red

#PREPROCESS RAW TEXT FOR SENTIMENT ANALYSIS (Why? tweets contain emojis etc.)
#What is preprocessing? : cleaning, formatting the data before feeding into a ML Algorithm
# Major steps? - Tokenizing the string, lowercasing, removing stop words and punctuation, stemming


#SELECTING ONE POSITIVE TWEET TO CHECK OUT ITS CONTENTS
tweet = all_positive_tweets[2277]
print(tweet)

#IMPORTING A FEW LIBRARIES FOR PREPROCESSING
nltk.download('stopwords')

import re #Library for regular expression operations
import string #for string operations

from nltk.corpus import stopwords #module for stop words that come with NLTK
from nltk.stem import PorterStemmer #module for stemming
from nltk.tokenize import TweetTokenizer #module for tokenizing strings

#REMOVE HYPERLINKS, TWITTER MARKS AND STYLES
#re(regular expressions) library #using the sub() method
print('\033[92m' + tweet)
print('\033[94m')

tweet2 = re.sub(r'^RT[\s]+',  '', tweet) #remove old style retweet text "RT"

re.sub(r'https?://[^\s\n\r]+', '', tweet2) #remove hyperlinks

tweet2 = re.sub(r'#', '', tweet2) #remove hashtags #only removing the hash # sign from the word

print(tweet2)

#TOKENIZE THE STRING USING THE "TOKENIZE" MODULE FROM NLTK
print()
print('\033[92m' + tweet2)
print('\033[94m')

tokenizer = TweetTokenizer(preserve_case = False, strip_handles = True, reduce_len = True) #instantiating tokenizer class
tweet_tokens = tokenizer.tokenize(tweet2) #tokenize tweets

print()
print('Tokenized string:')
print(tweet_tokens)

#REMOVE STOP WORDS AND PUNCTUATIONS
stopwords_english = stopwords.words('english')

print('Stop words\n')
print(stopwords_english)

print('\nPunctuation\n')
print(string.punctuation)

#CLEANING UP TOKENIZED TWEET

print()
print('\033[92m')
print(tweet_tokens)
print('\033[94m')

tweets_clean = []

for word in tweet_tokens: #Go through every word in your tokens list
  if (word not in stopwords_english and   #remove stopwords
      word not in string.punctuation):     #remove punctuation
      tweets_clean.append(word)

print('removed stop words and punctuation:')
print(tweets_clean)

#STEMMING
print()
print('\033[92m')
print(tweets_clean)
print('\022[94m')

stemmer = PorterStemmer() #instantiating stemming class #The PorterStemmer module uses the Porter Stemming Algorithm

tweets_stem = [] #Create an empty list to store the stems

for word in tweets_clean:
  stem_word = stemmer.stem(word) #stemming word
  tweets_stem.append(stem_word) #append to the list

print('stemmed words:')
print(tweets_stem)

#Calling the function process_tweet()
from utils import process_tweet #Importing the process_tweet function

tweet = all_positive_tweets[2277]

print()
print('\033[92m')
print(tweet)
print('033[94m')

#CALLING THE IMPORTED FUNCTION " "
tweets_stem = process_tweet(tweet); #Preprocess a given tweet

print('preprocessed tweet:')
print(tweets_stem) #Print the result















